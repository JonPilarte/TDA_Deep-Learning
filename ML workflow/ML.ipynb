{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2419804",
   "metadata": {},
   "source": [
    "# <font color='green'>1. Import needed packages</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad33f6",
   "metadata": {},
   "source": [
    "Additional needed packages:\n",
    "- imblearn\n",
    "- tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "041ad765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111b7a8",
   "metadata": {},
   "source": [
    "# <font color='green'>2. Importing and understanding the data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b5f5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_labels= pd.read_csv('all-ClinicalLabels-OLD_case_sensitive.csv', low_memory=False)\n",
    "df_inputs= pd.read_csv('dataset_Elderberry_plus_120nm_64_onh_vessel_removed.csv', low_memory=False)\n",
    "\n",
    "#display(df_labels) #FULL_ID\n",
    "#display(df_inputs) #Observations\n",
    "\n",
    "df_inputs = df_inputs.rename(columns={'Observations': 'FULL_ID'})\n",
    "merged_df = pd.merge(df_labels, df_inputs, on='FULL_ID', how='inner')\n",
    "display(merged_df)\n",
    "\n",
    "#print(df_labels['FULL_ID'].nunique())\n",
    "#print(df_inputs['FULL_ID'].nunique())\n",
    "print(merged_df['PATIENTID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c99382",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merged_df['PET_RESULT'].value_counts(sort=True, ascending=True).plot(kind='barh',color='green');\n",
    "plt.savefig('plot0.png', dpi=300, bbox_inches='tight',transparent =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1229a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['PET_RESULT'].value_counts(sort=True, ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff053f0",
   "metadata": {},
   "source": [
    "# <font color='green'>3. First manual Classifier and studying of the results </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4677d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics, preprocessing, tree\n",
    "from sklearn.metrics import make_scorer, SCORERS\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02b5cbe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# assume you have a pandas dataframe 'data' with a column 'target' and a column 'group' that you want to group on\n",
    "X = merged_df.drop(['PET_RESULT'], axis=1)\n",
    "y = merged_df['PET_RESULT']\n",
    "groups = merged_df['PATIENTID']\n",
    "\n",
    "# # perform undersampling with RandomUnderSampler\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "# X, y = rus.fit_resample(X, y, groups=groups)\n",
    "\n",
    "# perform oversampling with RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X, y = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea473e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Balancing data using the SMOTE oversampler to get better and less biased results\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# variables=merged_df.drop(['FULL_ID','PATIENTID','IOL','AGE','GENDER','EYE',\n",
    "#                          'COGNITION','HIS_SCORE','PET_RESULT',\n",
    "#                          'UNANIMOUS','APOE_GENOTYPE'], axis=1)\n",
    "# X= variables\n",
    "# y= merged_df.loc[:,'PET_RESULT']\n",
    "\n",
    "# #Balancing the data\n",
    "# #sampling = RandomUnderSampler(sampling_strategy='auto')\n",
    "# sampling = SMOTE(sampling_strategy='auto')\n",
    "# X, y = sampling.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707aae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(sort=True, ascending=True).plot(kind='barh',color='purple');\n",
    "plt.savefig('plot1.png', dpi=300, bbox_inches='tight',transparent =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd54b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf=StratifiedKFold(n_splits=5)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5) \n",
    "# # 80% training and 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d708cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "groups = X['PATIENTID']\n",
    "\n",
    "# create GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# perform cross-validation\n",
    "for train_idx, test_idx in gkf.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    # train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e326e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train['PATIENTID'].nunique())\n",
    "print(X_test['PATIENTID'].nunique())\n",
    "X_train=X_train.drop(['FULL_ID','IOL','AGE','GENDER','EYE',\n",
    "                          'COGNITION','HIS_SCORE','PET_RESULT',\n",
    "                          'UNANIMOUS','APOE_GENOTYPE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b87a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96eb15d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=18, svd_solver='arpack')\n",
    "X_train=pca.fit_transform(X_train)\n",
    "display(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280dbcc2",
   "metadata": {},
   "source": [
    "# <font color='green'>5. Automated Classifier and Hyperparameter tunning with TPOT-AutoML</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86167cf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Define TPOT classifier\n",
    "tpot_model = TPOTClassifier(generations=20, population_size=10, \n",
    "                            periodic_checkpoint_folder=\"tpot_mnst1.txt\", verbosity=2, \n",
    "                            random_state=42, cv = kf, scoring = 'accuracy', n_jobs=-1, config_dict=None) \n",
    "\n",
    "# Fit/start training\n",
    "tpot_model.fit(X_train, y_train)\n",
    "print('Done training/fitting TPOT session.')\n",
    "\n",
    "# Get TPOT's score on test set (default metric is 'accuracy'; define something else in TPOT classifier if needed)\n",
    "print('TPOTs score on test set is...')\n",
    "print(tpot_model.score(X_test, y_test))\n",
    "\n",
    "# Export the best pipeline\n",
    "tpot_model.export('tpot_best_pipeline.py')\n",
    "\n",
    "# Create sorted by CV (highest to lowest) dataframe \n",
    "my_dict = list(tpot_model.evaluated_individuals_.items())\n",
    "# Create an empty dataframe to append the model strings, model info strings and CV score strings to\n",
    "model_scores = pd.DataFrame()\n",
    "for model in my_dict:\n",
    "    model_name = model[0]\n",
    "    model_info = model[1] # You could take this out if the values of the pipeline aren't important to you\n",
    "    cv_score = model[1].get('internal_cv_score')  # Pull out cv_score as a column (i.e., sortable)\n",
    "    model_scores = model_scores.append({'model': model_name,\n",
    "                                        'cv_score': cv_score, # You could take this out if the values of the pipeline aren't important to you\n",
    "                                        'model_info': model_info,},\n",
    "                                       ignore_index=True)\n",
    "    \n",
    "# Sort by best CV score to worst (top to bottom)\n",
    "model_scores = model_scores.sort_values('cv_score', ascending=False)\n",
    "print('Model Scores dataframe is...')\n",
    "print(model_scores)\n",
    "\n",
    "# Remove duplicate CV score rows and keep top X pipelines (to get best, 'unique' pipelines)\n",
    "model_scores = model_scores.drop_duplicates(subset =\"cv_score\", keep = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1afbaea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from string import ascii_letters\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.set_theme(style=\"white\")\n",
    "\n",
    "# # Compute the correlation matrix\n",
    "# corr = df.drop(\"legitimate\", axis='columns').corr()\n",
    "\n",
    "# # Generate a mask for the upper triangle\n",
    "# mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# # Set up the matplotlib figure\n",
    "# f, ax = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "# # Generate a custom diverging colormap\n",
    "# cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# # Draw the heatmap with the mask and correct aspect ratio\n",
    "# fig=sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "#             square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "# plt.savefig('plot4.png', dpi=300, bbox_inches='tight',transparent =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48e7ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create Decision Tree classifer object\n",
    "# clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# # Train Decision Tree Classifer\n",
    "# clf = clf.fit(X_train,y_train)\n",
    "\n",
    "# print('Score: '+str(clf.score(X_test, y_test)))\n",
    "\n",
    "# #Predict the response for test dataset\n",
    "# y_pred = clf.predict_proba(X_test)\n",
    "# y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "# y_predX = clf.predict_proba(X)[::,1]\n",
    "# y_predent = clf.predict_proba(X_train)[::,1]\n",
    "\n",
    "# # print(y_pred) \n",
    "\n",
    "# fig, ax = plt.subplots(dpi=300)\n",
    "\n",
    "# disp = ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test,\n",
    "#                                  cmap=plt.cm.Blues, xticks_rotation='vertical', normalize=None, ax=ax)\n",
    "# disp.ax_.set_title(\"Confusion matrix, without normalization\")\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# #plt.savefig('plot5.png', dpi=300, bbox_inches='tight',transparent =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26bc9f",
   "metadata": {},
   "source": [
    "# <font color='green'>4. Manual Classifier and manual Hyperparameter tunning (RandomizedSearchCV)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47532855",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# kf=StratifiedKFold(n_splits=5)\n",
    "\n",
    "# models = [\n",
    "#     DecisionTreeClassifier(),\n",
    "#     RandomForestClassifier(n_jobs=-1),\n",
    "#     SVC()\n",
    "# ]\n",
    "\n",
    "# CV = 5\n",
    "# cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "# entries = []\n",
    "# for model in models:\n",
    "#   model_name = model.__class__.__name__\n",
    "#   accuracies = cross_val_score(model, X, y, scoring='accuracy', cv=kf)\n",
    "#   for fold_idx, accuracy in enumerate(accuracies):\n",
    "#     entries.append((model_name, fold_idx, accuracy))\n",
    "# cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "# cv_df\n",
    "\n",
    "# mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "# std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "# acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
    "#           ignore_index=True)\n",
    "# acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "# acc['Mean Accuracy'] = acc['Mean Accuracy'].astype(float).map(lambda n: '{:.2%}'.format(n))\n",
    "# acc['Standard deviation'] = acc['Standard deviation'].astype(float).map(lambda n: '{:.2%}'.format(n))\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Hyperparameters tuning with RandomizedSearchCV\n",
    "\n",
    "# n_estimators = [100,200,500,1000]\n",
    "# max_depth = [3, 4, 5, 6]\n",
    "# min_weight_fraction_leaf=list(np.arange(0.01, 0.12, 0.005))\n",
    "# criterion=['gini','entropy'] \n",
    "# max_features=['sqrt','log2', None]\n",
    "# bootstrap = [True,False]\n",
    "\n",
    "# hyperF = dict(max_depth = max_depth, \n",
    "#               n_estimators = n_estimators,   \n",
    "#               min_weight_fraction_leaf = min_weight_fraction_leaf,\n",
    "#               max_features = max_features,\n",
    "#               criterion = criterion, \n",
    "#               bootstrap = bootstrap)\n",
    "\n",
    "# est = RandomForestClassifier() \n",
    "\n",
    "# grid_obj=RandomizedSearchCV(est, hyperF, n_iter=5, scoring='accuracy',random_state=3,\n",
    "#                             n_jobs=-1, cv=kf, verbose = 2, return_train_score=True)\n",
    "\n",
    "# grid_fit=grid_obj.fit(X_train,y_train) \n",
    "\n",
    "# df_cv=pd.DataFrame.from_dict(grid_fit.cv_results_)\n",
    "# df_cv=df_cv.sort_values(by=['mean_test_score'], ascending=False)\n",
    "\n",
    "# print('Score CV Test: '+str(grid_fit.best_score_),\n",
    "#       '\\n','Score CV Training : '+str(df_cv['mean_train_score'][0]),\n",
    "#       '\\n',grid_fit.best_params_)\n",
    "\n",
    "# est = RandomForestClassifier(**grid_fit.best_params_, random_state = 5, verbose = 0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d144be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc48fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import ParameterGrid\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import parfit.parfit as pf\n",
    "\n",
    "# grid = {\n",
    "#     'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5], # learning rate\n",
    "#     'loss': ['log_loss'], # logistic regression\n",
    "#     'max_iter':[1000,2000,5000,10000],\n",
    "#     'penalty': ['l2']}\n",
    "\n",
    "# est = SGDClassifier() \n",
    "\n",
    "# grid_obj=RandomizedSearchCV(est, grid, n_iter=50, scoring='accuracy',random_state=3,\n",
    "#                             n_jobs=-1, cv=kf, verbose = 2, return_train_score=True)\n",
    "\n",
    "# grid_fit=grid_obj.fit(X_train,y_train) \n",
    "\n",
    "# df_cv=pd.DataFrame.from_dict(grid_fit.cv_results_)\n",
    "# df_cv=df_cv.sort_values(by=['mean_test_score'], ascending=False)\n",
    "\n",
    "# print('Score CV Test: '+str(grid_fit.best_score_),\n",
    "#       '\\n','Score CV Training : '+str(df_cv['mean_train_score'][0]),\n",
    "#       '\\n',grid_fit.best_params_)\n",
    "\n",
    "# est = SGDClassifier(**grid_fit.best_params_, random_state = 5, verbose = 0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a75c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
